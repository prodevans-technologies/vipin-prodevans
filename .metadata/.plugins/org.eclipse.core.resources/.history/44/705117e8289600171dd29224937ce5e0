package com.prodevans.scala


import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.SparkConf
import org.apache.spark.sql.SQLContext

object DataFormat 
{
  
  def main(args: Array[String])
  {
  
    val conf= new SparkConf().setAppName("Join Operation").setMaster("local").set("spark.hadoop.validateOutputSpecs","false")
    val sc=new SparkContext(conf)
    
    val rawRDD= sc.textFile("",2)
    
    val rawRDDEmployees = rawRDD.map(record => record.split(",",-1))
                                .map( x=> CaseClassEmployees 
                                          (
                                                Integer.parseInt(x(0).toString()),
                                                x(1),
                                                x(2),
                                                x(3),
                                                Integer.parseInt(x(4).toString()),
                                                x(5)
                                          )
                                     )
    
    val sqlContext=new SQLContext(sc)
    
   
                                     
                                     
  }

}