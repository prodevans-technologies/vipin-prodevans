package com.prodevans.it.SparkPorgrams

import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.SparkConf

object BroadcastFilter 
{

  
   def main(args:Array[String])
  {
    
      val conf= new SparkConf().setAppName("Broadcast filter").setMaster("local").set("spark.hadoop.validateOutputSpecs","false")
      val sc=new SparkContext(conf)
      
      val contentRDD=sc.textFile("C:/Users/vip/Desktop/abc.txt")
      
      val words=contentRDD.flatMap( line => line.split(",") )

      val removeRDD = sc.textFile("C:/Users/vip/Desktop/remove.txt")
                        .flatMap( x => x.split(",",-1))
                        .map( y => y.trim())
                        
      val bRemove=sc.broadcast(removeRDD.collect().toList)
      
      val filteredRDD  = words.filter
                               { x => x match
                                      {
                                         case x=> !bRemove.value.contains(x)
                                      }
                               }
      val pariRDD = filteredRDD.map( x=>(x,1))
      val wordCount=pariRDD.reduceByKey(_ + _)
      
      val finalResult=wordCount.saveAsTextFile("C:/Users/vip/Desktop/target.txt")
      
      
  }
  
  
}