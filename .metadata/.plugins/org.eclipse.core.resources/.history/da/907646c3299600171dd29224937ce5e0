package com.prodevans.scala


import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.SparkConf
import org.apache.spark.sql.SQLContext

object DataFormat 
{
  
  def main(args: Array[String])
  {
  
    System.setProperty("hadoop.home.dir", "C:/Users/vip/Documents/GitHub/Hadoop/winutils");
    
    val conf= new SparkConf().setAppName("Join Operation").setMaster("local").set("spark.hadoop.validateOutputSpecs","false")
    val sc=new SparkContext(conf)
    
    val rawRDD= sc.textFile("",2)
    
    val rawRDDEmployees = rawRDD.map(record => record.split(",",-1))
                                .map( x=> CaseClassEmployees 
                                          (
                                                Integer.parseInt(x(0).toString()),
                                                x(1),
                                                x(2),
                                                x(3),
                                                Integer.parseInt(x(4).toString()),
                                                x(5)
                                          )
                                     )
    
    val sqlContext=new SQLContext(sc)
    
    import sqlContext.implicits._
    
    val rawDF=rawRDDEmployees.toDF()
    
    rawDF.groupBy("emp_dept").count().show()
    
    rawDF.registerTempTable("Employees")
    
    val querDF=sqlContext.sql("select emp_name,emp_sal from Employees") 
                                     
    querDF.show()                                 
  }

}